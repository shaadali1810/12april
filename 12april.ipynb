{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b90b032-9486-46c6-bdea-1c20e78cbf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Bagging, also known as bootstrap aggregating, is the process of creating multiple trees from different subsets of the training data, and then averaging or voting their predictions. Bagging can help reduce the variance and the noise of individual trees, and increase the diversity and stability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0412ae0a-2bca-422c-90c2-7b80ea05800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 The biggest advantage of bagging is that multiple weak learners can work better than a single strong learner. It provides stability and increases the machine learning algorithm's accuracy, which is used in statistical classification and regression.\n",
    "#agging offers the advantage of allowing many weak learners to combine efforts to outdo a single strong learner. It also helps in the reduction of variance, hence eliminating the overfitting of models in the procedure. One disadvantage of bagging is that it introduces a loss of interpretability of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec9a210a-6b7d-43f8-b707-2cd87314f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3Bagging aims to decrease variance, boosting aims to decrease bias, and stacking aims to improve prediction accuracy. Bagging and boosting combine homogenous weak learners. Stacking combines heterogeneous solid learners. Bagging trains models in parallel and boosting trains the models sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c699b905-4622-4b62-ab89-4613f5c57d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 Ensemble methods can be used for both classification and regression tasks and have been shown to improve the performance of models significantly in many real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e6d95b8-a45e-48ec-9566-eb5096210686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 Mostly statistical tests were used for determining the proper number of components. More recently, a theoretical framework suggested that there is an ideal number of component classifiers for an ensemble such that having more or less than this number of classifiers would deteriorate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bccb59-75a9-4a38-9b90-eb3baae7b46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
